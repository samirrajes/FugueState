{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6fb783",
   "metadata": {},
   "source": [
    "## 1. Setup, Imports & Configuration\n",
    "\n",
    "These cell loads all required libraries and utilities for audio processing, modeling, visualization, and inline playback.\n",
    "\n",
    "We configure project paths, device (MPS/CPU), audio/spectrogram parameters, and training hyperparameters. Also import our custom `src/` modules (`ChordSpec`, `AblationGenerator128`, `train_gan`, `spec_to_audio`).  \n",
    "\n",
    "### Note: Ensure you run these 2 cells before proceeding with the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8611f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import fftconvolve\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from IPython.display import Audio, display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703942b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confguration of paths\n",
    "NOTEBOOK_DIR = os.getcwd()\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(NOTEBOOK_DIR, os.pardir))\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# device selection: MPS (Apple Silicon) if available, otherwise CPU\n",
    "# all development of this project was done on an Apple Silicon Mac, if you have an NVIDIA GPU, please adjust the code to use CUDA.\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# setting up paths\n",
    "DATA_ROOT   = os.path.join(PROJECT_ROOT, \"data\", \"guitar\", \"Training\")\n",
    "OUTPUT_DIR  = os.path.join(PROJECT_ROOT, \"checkpoints\", \"guitar\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Training data root: {DATA_ROOT}\")\n",
    "print(f\"Checkpoints: {OUTPUT_DIR}\")\n",
    "\n",
    "# Note: ChatGPT was used to research and set the following audio parameters\n",
    "\n",
    "# audio & spectrogram parameters\n",
    "# these parameters are all set to convential values\n",
    "SR = 22050 # sampling rate\n",
    "N_FFT = 1024 # fft window size\n",
    "HOP_LENGTH = 256 # hop (stride) for stft\n",
    "N_MELS = 128 # number of mel bands\n",
    "FMIN = 50.0 # min freq for mel scale\n",
    "FMAX = 8000.0 # max freq for mel scale\n",
    "CLIP_DUR = 3.0 # clip duration in seconds\n",
    "\n",
    "# training hyperparameters\n",
    "BATCH_SIZE = 16\n",
    "LATENT_DIM = 100\n",
    "LR = 2e-4\n",
    "BETA1, BETA2 = 0.5, 0.999\n",
    "EPOCHS = 500\n",
    "CHECKPOINT_EPOCH = 100\n",
    "\n",
    "# project modules\n",
    "from src.data.dataset import ChordSpec\n",
    "from src.models import AblationGenerator128\n",
    "from src.train import train_gan\n",
    "from src.utils import spec_to_audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3e969",
   "metadata": {},
   "source": [
    "## 2. Dataset Inspection & Spectrogram Reconstruction\n",
    "\n",
    "This cell:\n",
    "\n",
    "1. Loads the `ChordSpec` dataset and pick a random example.  \n",
    "2. Visualizes its 128-band mel-spectrogram.  \n",
    "3. Inverts the spectrogram back to audio using our `spec_to_audio` Griffin–Lim routine.  \n",
    "4. Plays the original and reconstructed waveforms side by side for comparison.  \n",
    "\n",
    "Run this cell to verify data loader and inversion pipeline before proceeding.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1275c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate dataset & pick one random sample wav\n",
    "dataset = ChordSpec(\n",
    "    root=DATA_ROOT,\n",
    "    sr=SR,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH,\n",
    "    n_mels=N_MELS,\n",
    "    fmin=FMIN,\n",
    "    fmax=FMAX,\n",
    "    duration=CLIP_DUR\n",
    ")\n",
    "idx = np.random.randint(len(dataset))\n",
    "\n",
    "# get & plot its mel-spectrogram\n",
    "spec = dataset[idx].squeeze(0).numpy()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.imshow(spec, origin='lower', aspect='auto')\n",
    "plt.title('Random Mel-spectrogram sample')\n",
    "plt.colorbar(label='Normalized dB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# load original waveform\n",
    "file_path = dataset.files[idx]\n",
    "y_orig, _ = librosa.load(file_path, sr=SR, mono=True, duration=CLIP_DUR)\n",
    "\n",
    "# reconstruct via Griffin–Lim utility, code for this function can be found in src/utils.py\n",
    "y_recon = spec_to_audio(\n",
    "    spec,\n",
    "    sr=SR,\n",
    "    n_fft=N_FFT,\n",
    "    hop_length=HOP_LENGTH\n",
    ")\n",
    "\n",
    "# display and play both\n",
    "print(\"Original:\")\n",
    "display(Audio(y_orig, rate=SR))\n",
    "print(\"Reconstructed:\")\n",
    "display(Audio(y_recon, rate=SR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47406421",
   "metadata": {},
   "source": [
    "## 3. Optional: Train GAN\n",
    "\n",
    "You can load any of the checkpoints available in `./checkpoints/guitar/` for epochs 100, 200, 300, and 400 and skip training. \n",
    "\n",
    "- **Skip training:** Move on to the next section to load a pretrained generator.\n",
    "- **To train from scratch:** Uncomment the `train_gan(...)` call in the last line of the cell blow and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad4b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader setup\n",
    "loader = DataLoader(\n",
    "    ChordSpec(\n",
    "        root=DATA_ROOT,\n",
    "        sr=SR,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS,\n",
    "        fmin=FMIN,\n",
    "        fmax=FMAX,\n",
    "        duration=CLIP_DUR\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=(DEVICE != torch.device('cpu'))\n",
    ")\n",
    "\n",
    "# training config\n",
    "cfg = {\n",
    "    'latent_dim': LATENT_DIM,\n",
    "    'feature_maps': 64,\n",
    "    'lr': LR,\n",
    "    'beta1': BETA1,\n",
    "    'beta2': BETA2,\n",
    "    'epochs': EPOCHS,\n",
    "    'n_mels': N_MELS,\n",
    "    'checkpoint_epoch': CHECKPOINT_EPOCH\n",
    "}\n",
    "\n",
    "# run training loop. uncomment the line below if you wish to train the GAN\n",
    "#G = train_gan(loader, DEVICE, OUTPUT_DIR, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c48df5",
   "metadata": {},
   "source": [
    "## 4. Inference: Generate & Listen to Samples\n",
    "\n",
    "This cell loads the pretrained generator from `G_ep400.pth` and produces five random spectrograms along with their audio reconstructions.\n",
    "\n",
    "1. Load checkpoint: Instantiates `AblationGenerator128` and loads weights from epoch 400.  \n",
    "2. Sample latent vectors: Draws 5 random `z` vectors and runs them through the generator.  \n",
    "3. Visualize: Plots each generated mel-spectrogram.  \n",
    "4. Listen: Uses `spec_to_audio` to invert each spectrogram and plays back the resulting waveform.  \n",
    "\n",
    "Feel free to rerun this cell to hear different random samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be7f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pretrained generator\n",
    "G = AblationGenerator128(latent_dim=LATENT_DIM, feature_maps=64, mask=None).to(DEVICE)\n",
    "ckpt_path = os.path.join(OUTPUT_DIR, \"G_ep400.pth\")\n",
    "G.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
    "G.eval()\n",
    "\n",
    "# sample 5 random spectrograms\n",
    "zs = torch.randn(5, LATENT_DIM, 1, 1, device=DEVICE)\n",
    "with torch.no_grad():\n",
    "    specs, _ = G(zs)\n",
    "specs_np = specs.squeeze(1).cpu().numpy()\n",
    "\n",
    "# plot the generated spectrograms\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(specs_np[i], origin='lower', aspect='auto')\n",
    "    ax.set_title(f\"Sample {i+1}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# reconstruct audio from generated spectrograms and play them\n",
    "for i, spec in enumerate(specs_np, start=1):\n",
    "    y = spec_to_audio(\n",
    "        spec,\n",
    "        sr=SR,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        win_length=N_FFT\n",
    "    )\n",
    "    print(f\"Play generated sample {i}\")\n",
    "    display(Audio(y, rate=SR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a863e0",
   "metadata": {},
   "source": [
    "## 5. Ablation Soundscape: Progressive Neuron Death\n",
    "\n",
    "This cell synthesizes the full “memory‐loss” soundscape by:\n",
    "\n",
    "1. **Loading the pretrained generator**\n",
    "2. **Defining ablation parameters**: zeroing out 5 random channels per segment.  \n",
    "3. **Building cumulative masks**: we shuffle all `(layer, channel)` pairs, chunk them into groups of 5, and accumulate the masks over segments (stored in `ablation_info`).  \n",
    "4. **Generating segments**: for each mask, we draw a fresh random latent vector `z`, apply the mask in `Gm.mask`, generate its mel-spectrogram, and invert it via `spec_to_audio`.  \n",
    "5. **Segment stitching**: we overlap consecutive segments slightly to create one continuous waveform.\n",
    "6. **Optional reverb**: an exponential‐decay impulse response adds some depth/reverb if desired.  \n",
    "7. **Playback**: listen to the dry soundscape, and the added reverb one side by side.\n",
    "\n",
    "Run this cell to generate the soundscape and experience the gradual auditory degradation as neurons are “killed” in the generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ca27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.signal import fftconvolve\n",
    "from IPython.display import Audio\n",
    "\n",
    "# assume these are defined elsewhere or set appropriately:\n",
    "# LATENT_DIM, DEVICE, PROJECT_ROOT, SR, N_FFT, HOP_LENGTH, spec_to_audio\n",
    "\n",
    "# Note: ChatGPT was used to debug and sanity check the ablation mask code\n",
    "\n",
    "# load pretrained generator for ablation study\n",
    "Gm = AblationGenerator128(latent_dim=LATENT_DIM, feature_maps=64, mask=None).to(DEVICE)\n",
    "ckpt = torch.load(\n",
    "    os.path.join(PROJECT_ROOT, \"checkpoints\", \"guitar\", \"G_ep400.pth\"),\n",
    "    map_location=DEVICE\n",
    ")\n",
    "Gm.load_state_dict(ckpt)\n",
    "Gm.eval()\n",
    "\n",
    "# ablation parameters\n",
    "kills_per_segment = 5\n",
    "\n",
    "# build deconv neuron list & random kill order\n",
    "deconv_layers = [\n",
    "    (i, layer) for i, layer in enumerate(Gm.net)\n",
    "    if isinstance(layer, torch.nn.ConvTranspose2d)\n",
    "]\n",
    "neurons = [\n",
    "    (li, c)\n",
    "    for li, layer in deconv_layers\n",
    "    for c in range(layer.out_channels)\n",
    "]\n",
    "random.shuffle(neurons)\n",
    "\n",
    "# build cumulative masks for ablation\n",
    "chunks = [\n",
    "    neurons[i : i + kills_per_segment]\n",
    "    for i in range(0, len(neurons), kills_per_segment)\n",
    "]\n",
    "used, masks = set(), []\n",
    "for chunk in chunks:\n",
    "    used.update(chunk)\n",
    "    masks.append({\n",
    "        l: [c for (l0, c) in used if l0 == l]\n",
    "        for (l, _) in deconv_layers\n",
    "        if any(l0 == l for (l0, _) in used)\n",
    "    })\n",
    "ablation_info = {\"order\": neurons, \"masks\": masks}\n",
    "\n",
    "# generate one random z per segment, apply mask, invert to audio\n",
    "segments = []\n",
    "for mask in masks:\n",
    "    z = torch.randn(1, LATENT_DIM, 1, 1, device=DEVICE)\n",
    "    Gm.mask = mask\n",
    "    with torch.no_grad():\n",
    "        spec_t, _ = Gm(z)\n",
    "    spec = spec_t.cpu().squeeze().squeeze().numpy()\n",
    "    y = spec_to_audio(spec, sr=SR, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    segments.append(y)\n",
    "\n",
    "# stitch together with 30% overlap between consecutive segments\n",
    "overlap = int(len(segments[0]) * 0.30) \n",
    "out = segments[0].copy()\n",
    "for seg in segments[1:]:\n",
    "    # sum overlap region\n",
    "    overlapped = out[-overlap:] + seg[:overlap]\n",
    "    out = np.concatenate([\n",
    "        out[:-overlap],\n",
    "        overlapped,\n",
    "        seg[overlap:]\n",
    "    ])\n",
    "\n",
    "# normalize to prevent clipping\n",
    "out /= np.max(np.abs(out))\n",
    "\n",
    "print(f\"Total duration: {len(out)/SR:.2f}s, Segments: {len(segments)}\")\n",
    "\n",
    "# apply slight reverb to the output\n",
    "reverb_secs = 0.2\n",
    "decay_rate  = 4.0\n",
    "ir_len      = int(reverb_secs * SR)\n",
    "t_ir        = np.linspace(0, reverb_secs, ir_len)\n",
    "ir = np.exp(-decay_rate * t_ir)\n",
    "ir[0] = 1.0\n",
    "\n",
    "wet = fftconvolve(out, ir, mode=\"full\")[:len(out)]\n",
    "wet /= np.max(np.abs(wet))\n",
    "\n",
    "# playback\n",
    "print(\"Dry (no reverb)\")\n",
    "display(Audio(out, rate=SR))\n",
    "\n",
    "print(\"Wet (with reverb)\")\n",
    "display(Audio(wet, rate=SR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f91cd0",
   "metadata": {},
   "source": [
    "## 6. Neuron Ablation Animation\n",
    "\n",
    "This cell visualizes the sequence of neuron “deaths” in the generator as a synchronized animation:\n",
    "\n",
    "1. **Layout the neuron grid**: each deconvolution channel is drawn as a small white circle positioned by layer (y-axis) and channel index (x-axis).  \n",
    "2. **Compile kill indices**: frame 0 shows all neurons alive; each subsequent frame highlights the newly ablated neurons in red.  \n",
    "3. **Matplotlib scatter plot** \n",
    "4. **Animate with `FuncAnimation`** at the same tempo as the soundscape segments, so the visual and audio degradations stay in sync.  \n",
    "\n",
    "Run this cell _after_ generating the soundscape to see network degradation alongside audio playback. You can see exactly when and where neurons are being switched off in the network’s latent-to-spectrogram pipeline.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ec2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build neuron grid\n",
    "layer_sizes = [layer.out_channels for _,layer in deconv_layers]\n",
    "n_layers = len(layer_sizes)\n",
    "neuron_list, xs, ys, ss = [], [], [], []\n",
    "for depth, (li, size) in enumerate(zip([i for i,_ in deconv_layers], layer_sizes)):\n",
    "    for c in range(size):\n",
    "        neuron_list.append((li, c))\n",
    "        xs.append((c + 0.5)/size)\n",
    "        ys.append(1 - (depth + 0.5)/n_layers)\n",
    "        ss.append(max(0.5, 500/size))\n",
    "\n",
    "# build kill indices per frame\n",
    "kill_by_frame = [[]]\n",
    "for mask in masks:\n",
    "    killed = [(l,c) for l,chans in mask.items() for c in chans]\n",
    "    kill_by_frame.append([neuron_list.index(n) for n in killed])\n",
    "total_frames = len(kill_by_frame)\n",
    "\n",
    "# plot the setup\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "fig.patch.set_facecolor('black'); ax.set_facecolor('black')\n",
    "ax.axis('off'); ax.set_xlim(0,1); ax.set_ylim(0,1)\n",
    "scat = ax.scatter(xs, ys, c='white', s=ss, edgecolors='none')\n",
    "\n",
    "# update frame function\n",
    "def update(frame):\n",
    "    dead = set(kill_by_frame[frame])\n",
    "    colors = ['red' if i in dead else 'white' for i in range(len(neuron_list))]\n",
    "    scat.set_color(colors)\n",
    "    return scat,\n",
    "\n",
    "# one frame per segment\n",
    "interval_ms = int((len(segments[0])*(1-0.3))/SR*1000)  # approx segment play time\n",
    "anim = FuncAnimation(fig, update, frames=total_frames, interval=interval_ms, blit=True)\n",
    "\n",
    "# display the animation\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e43005",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../animations'\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "fps = 1000 / interval_ms\n",
    "\n",
    "anim.save(os.path.join(out_dir, 'ablation_animation.mp4'), writer='ffmpeg', fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sound-gan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
